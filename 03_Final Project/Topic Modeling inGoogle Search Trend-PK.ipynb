{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n",
    "\n",
    "- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
    "- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
    "- **Tagging**, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
    "\n",
    "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "In this tutorial, we’ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n",
    "\n",
    "### Theoretical Overview\n",
    "LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
    "\n",
    "![LDA_Model](https://github.com/chdoig/pytexas2015-topic-modeling/blob/master/images/lda-4.png?raw=true)\n",
    "\n",
    "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
    "\n",
    "- `psi`, the distribution of words for each topic K\n",
    "- `phi`, the distribution of topics for each document i\n",
    "\n",
    "#### Parameters of LDA\n",
    "\n",
    "- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
    "- `Beta parameter` is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Implementation\n",
    "\n",
    "1. Loading data\n",
    "2. Data cleaning\n",
    "3. Exploratory analysis\n",
    "4. Preparing data for LDA analysis\n",
    "5. Model Evaluation and parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data\n",
    "\n",
    "For this tutorial, we’ll use the dataset of papers published in NIPS conference. The NIPS conference (Neural Information Processing Systems) is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
    "\n",
    "Let’s start by looking at the content of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Read data from MongoDB\n",
    "cluster = MongoClient('mongodb+srv://rimho:0000@cluster0-yehww.mongodb.net/test?retryWrites=true&w=majority')\n",
    "db = cluster['3_Google_search_trends_db']\n",
    "collections = [db[c] for c in ['IN']]\n",
    "documents =  [collection.find() for collection in collections]\n",
    "\n",
    "products = []\n",
    "for document in documents:\n",
    "    for p in document:\n",
    "        products.append(p)\n",
    "table = json_normalize(products)\n",
    "\n",
    "# # Print head\n",
    "table.head()\n",
    "\n",
    "\n",
    "#### Data Cleaning\n",
    "\n",
    "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns\n",
    "\n",
    "# Remove the columns\n",
    "columns_name = list(table)\n",
    "basic_df = table.drop(table.columns[[0, 6]], axis=1)\n",
    "cols = basic_df.columns.tolist()\n",
    "df = basic_df[cols]#.sort_values(by='year', ascending=False)\n",
    "keyword_PK = df.loc[df['country'] == 'PK']\n",
    "\n",
    "# Print out the first rows of papers\n",
    "df\n",
    "\n",
    "#### Remove punctuation/lower casing\n",
    "\n",
    "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text\n",
    "\n",
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "keyword_PK['keyword_processed'] = keyword_PK['keyword'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Convert the titles to lowercase\n",
    "keyword_PK['keyword_processed'] = keyword_PK['keyword'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "keyword_PK['keyword_processed'].head()\n",
    "\n",
    "keyword_PK_2014 = keyword_PK.loc[df['year'] == '2014']\n",
    "keyword_PK_2015 = keyword_PK.loc[df['year'] == '2015']\n",
    "keyword_PK_2016 = keyword_PK.loc[df['year'] == '2016']\n",
    "keyword_PK_2017 = keyword_PK.loc[df['year'] == '2017']\n",
    "keyword_PK_2018 = keyword_PK.loc[df['year'] == '2018']\n",
    "keyword_PK_2014\n",
    "\n",
    "#### Exploratory Analysis\n",
    "\n",
    "To verify whether the preprocessing happened correctly, we’ll make a word cloud using the wordcloud package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model.\n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(keyword_PK_2014['keyword_processed'].values)) \n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, colormap=matplotlib.cm.twilight_shifted)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()\n",
    "\n",
    "long_string = ','.join(list(keyword_PK_2015['keyword_processed'].values))\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, colormap=matplotlib.cm.twilight_shifted)\n",
    "wordcloud.generate(long_string)\n",
    "wordcloud.to_image()\n",
    "\n",
    "long_string = ','.join(list(keyword_PK_2016['keyword_processed'].values))\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, colormap=matplotlib.cm.twilight_shifted)\n",
    "wordcloud.generate(long_string)\n",
    "wordcloud.to_image()\n",
    "\n",
    "long_string = ','.join(list(keyword_PK_2017['keyword_processed'].values))\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, colormap=matplotlib.cm.twilight_shifted)\n",
    "wordcloud.generate(long_string)\n",
    "wordcloud.to_image()\n",
    "\n",
    "long_string = ','.join(list(keyword_PK_2018['keyword_processed'].values))\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, colormap=matplotlib.cm.twilight_shifted)\n",
    "wordcloud.generate(long_string)\n",
    "wordcloud.to_image()\n",
    "\n",
    "#### Prepare text for LDA analysis\n",
    "\n",
    "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by converting the documents into a simple vector representation (Bag of Words BOW). Next, we will convert a list of titles into lists of vectors, all with length equal to the vocabulary.\n",
    "\n",
    "We’ll then plot the ten most frequent words based on the outcome of this operation (the list of document vectors). As a check, these words should also occur in the word cloud.\n",
    "\n",
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:30]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='30 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette=\"GnBu_d\")\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "    print(words)\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "\n",
    "keyword_PK_2018.loc[keyword_PK_2018['keyword_processed'].str.contains('brewery', regex=False)]\n",
    "\n",
    "PK_top_30 = ['khan', '2016', 'bigg', 'boss', 'cup', 'world', 'vs', 'icc', 'pakistan', 'live', 'ptv', 'sports', 'champions', 'express', 'mir', 'momina', 'mustehsan', 'news', 'psl', 'reham', 'samaa', 'schedule', 'trophy', 'trump', 'tv', 'tiger', 'baloch', 'fast', 'furious', 'qandeel']\n",
    "\n",
    "def find_category(df, arr):\n",
    "    new_arr = []\n",
    "    for kw in arr:\n",
    "        collection = {}\n",
    "        new_df = df.loc[df['keyword_processed'].str.contains(kw, regex=False)]\n",
    "        new_df = new_df.loc[new_df['search_volume'] == '100']\n",
    "        collection['keyword'] = kw\n",
    "        collection['year'] = list(new_df['year'])\n",
    "        collection['category'] = list(new_df['category'])\n",
    "        collection['all_keyword'] = list(new_df['keyword'])\n",
    "        collection['region'] = list(new_df['region'])\n",
    "        new_arr.append(collection)\n",
    "    return new_arr\n",
    "\n",
    "results = find_category(keyword_PK, PK_top_30)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['year'] = results_df['year'].apply(lambda x: ','.join(map(str, x)))\n",
    "results_df['category'] = results_df['category'].apply(lambda x: ','.join(map(str, x)))\n",
    "results_df['all_keyword'] = results_df['all_keyword'].apply(lambda x: ','.join(map(str, x)))\n",
    "results_df['region'] = results_df['region'].apply(lambda x: ','.join(map(str, x)))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>year</th>\n",
       "      <th>category</th>\n",
       "      <th>all_keyword</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>khan</td>\n",
       "      <td>2014,2014,2014,2015,2018,2018</td>\n",
       "      <td>People,People,People,People,People,People</td>\n",
       "      <td>Ayeza Khan,Sana Khan,Gauhar Khan,Reham Khan,Re...</td>\n",
       "      <td>Sindh,Sindh,Sindh,Khyber Pakhtunkhwa,Khyber Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bigg</td>\n",
       "      <td>2014,2015,2015,2016</td>\n",
       "      <td>Searches,Searches,Searches,Searches</td>\n",
       "      <td>Bigg Boss 8,Bigg Boss 8,Bigg Boss 9,Bigg Boss 9</td>\n",
       "      <td>Sindh,Balochistan,Balochistan,Balochistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>boss</td>\n",
       "      <td>2014,2015,2015,2016</td>\n",
       "      <td>Searches,Searches,Searches,Searches</td>\n",
       "      <td>Bigg Boss 8,Bigg Boss 8,Bigg Boss 9,Bigg Boss 9</td>\n",
       "      <td>Sindh,Balochistan,Balochistan,Balochistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>mir</td>\n",
       "      <td>2014,2015,2017,2017</td>\n",
       "      <td>People,People,People,People</td>\n",
       "      <td>Hamid Mir,Mira Rajput,Ahad Raza Mir,Hania Amir</td>\n",
       "      <td>Balochistan,Sindh,Azad Jammu and Kashmir,Khybe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>samaa</td>\n",
       "      <td>2014,2018</td>\n",
       "      <td>Searches,Searches</td>\n",
       "      <td>Samaa TV,Samaa TV</td>\n",
       "      <td>Balochistan,Sindh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>tv</td>\n",
       "      <td>2014,2014,2016,2018,2018</td>\n",
       "      <td>Searches,Searches,Searches,Searches,Searches</td>\n",
       "      <td>PTV Sports,Samaa TV,PTV Sports Live,PTV Sports...</td>\n",
       "      <td>Balochistan,Balochistan,Federally Administered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>tiger</td>\n",
       "      <td>2014,2018</td>\n",
       "      <td>People,Movies</td>\n",
       "      <td>Tiger Shroff,Tiger Zinda Hai</td>\n",
       "      <td>Sindh,Federally Administered Tribal Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>fast</td>\n",
       "      <td>2015,2017</td>\n",
       "      <td>Searches,Searches</td>\n",
       "      <td>Fast &amp; Furious 7,Fast and Furious 8</td>\n",
       "      <td>Sindh,Punjab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>furious</td>\n",
       "      <td>2015,2017</td>\n",
       "      <td>Searches,Searches</td>\n",
       "      <td>Fast &amp; Furious 7,Fast and Furious 8</td>\n",
       "      <td>Sindh,Punjab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    keyword                           year  \\\n",
       "0      khan  2014,2014,2014,2015,2018,2018   \n",
       "2      bigg            2014,2015,2015,2016   \n",
       "3      boss            2014,2015,2015,2016   \n",
       "14      mir            2014,2015,2017,2017   \n",
       "20    samaa                      2014,2018   \n",
       "24       tv       2014,2014,2016,2018,2018   \n",
       "25    tiger                      2014,2018   \n",
       "27     fast                      2015,2017   \n",
       "28  furious                      2015,2017   \n",
       "\n",
       "                                        category  \\\n",
       "0      People,People,People,People,People,People   \n",
       "2            Searches,Searches,Searches,Searches   \n",
       "3            Searches,Searches,Searches,Searches   \n",
       "14                   People,People,People,People   \n",
       "20                             Searches,Searches   \n",
       "24  Searches,Searches,Searches,Searches,Searches   \n",
       "25                                 People,Movies   \n",
       "27                             Searches,Searches   \n",
       "28                             Searches,Searches   \n",
       "\n",
       "                                          all_keyword  \\\n",
       "0   Ayeza Khan,Sana Khan,Gauhar Khan,Reham Khan,Re...   \n",
       "2     Bigg Boss 8,Bigg Boss 8,Bigg Boss 9,Bigg Boss 9   \n",
       "3     Bigg Boss 8,Bigg Boss 8,Bigg Boss 9,Bigg Boss 9   \n",
       "14     Hamid Mir,Mira Rajput,Ahad Raza Mir,Hania Amir   \n",
       "20                                  Samaa TV,Samaa TV   \n",
       "24  PTV Sports,Samaa TV,PTV Sports Live,PTV Sports...   \n",
       "25                       Tiger Shroff,Tiger Zinda Hai   \n",
       "27                Fast & Furious 7,Fast and Furious 8   \n",
       "28                Fast & Furious 7,Fast and Furious 8   \n",
       "\n",
       "                                               region  \n",
       "0   Sindh,Sindh,Sindh,Khyber Pakhtunkhwa,Khyber Pa...  \n",
       "2           Sindh,Balochistan,Balochistan,Balochistan  \n",
       "3           Sindh,Balochistan,Balochistan,Balochistan  \n",
       "14  Balochistan,Sindh,Azad Jammu and Kashmir,Khybe...  \n",
       "20                                  Balochistan,Sindh  \n",
       "24  Balochistan,Balochistan,Federally Administered...  \n",
       "25          Sindh,Federally Administered Tribal Areas  \n",
       "27                                       Sindh,Punjab  \n",
       "28                                       Sindh,Punjab  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[results_df['region'].str.contains('Sindh', regex=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.77777777777779"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 9\n",
    "target = 7\n",
    "(1-((total - target)/total)) *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model tranining\n",
    "\n",
    "To keep things simple, we will only tweak the number of topic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "tiger shroff yeh mohabbatein\n",
      "\n",
      "Topic #1:\n",
      "khan tv samaa ptv\n",
      "\n",
      "Topic #2:\n",
      "2014 world cup shah\n",
      "\n",
      "Topic #3:\n",
      "baby doll song boss\n",
      "\n",
      "Topic #4:\n",
      "live news express ary\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below (use int values below 15)\n",
    "number_topics = 5\n",
    "number_words = 4\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(count_data)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4f72bd300887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "list(result[0]['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our LDA model\n",
    "\n",
    "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
    "\n",
    "1. Better understanding and interpreting individual topics, and\n",
    "2. Better understanding the relationships between the topics.\n",
    "\n",
    "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
    "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "\n",
    "    LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, \"rb\") as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Notes\n",
    "Machine learning has become increasingly popular over the past decade, and recent advances in computational availability have led to exponential growth to people looking for ways how new methods can be incorporated to advance the field of Natural Language Processing.\n",
    "\n",
    "Often, we treat topic models as black-box algorithms, but hopefully, this post addressed to shed light on the underlying math, and intuitions behind it, and high-level code to get you started with any textual data.\n",
    "\n",
    "In the next article, we’ll go one step deeper into understanding how you can evaluate the performance of topic models, tune its hyper-parameters to get more intuitive and reliable results.\n",
    "\n",
    "** **\n",
    "#### Sources:\n",
    "1. Topic model — Wikipedia. https://en.wikipedia.org/wiki/Topic_model\n",
    "2. Distributed Strategies for Topic Modeling. https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&isAllowed=y\n",
    "3. Topic Mapping — Software — Resources — Amaral Lab. https://amaral.northwestern.edu/resources/software/topic-mapping\n",
    "4. A Survey of Topic Modeling in Text Mining. https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
